{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import seaborn as sns\n",
    "import jieba\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01.读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hole_df = pd.read_csv(\"../data/2023-06-08_holes_day.csv\", index_col=0)\n",
    "hole_df.dropna(inplace=True)\n",
    "hole_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_df['reply'] = hole_df.reply.astype(float)\n",
    "hole_df['likenum'] = hole_df.likenum.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_df['hot'] = hole_df.likenum*hole_df.reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_df.sort_values('hot').tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_df.loc[:,'hour'] = hole_df.time.apply(lambda x: x.split()[1].split(':')[0]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_df.groupby('hour').count().text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.lineplot(hole_df.groupby('hour').count().text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(hole_df.groupby('hour').hot.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hole_df = hole_df[hole_df['hot'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "#由于筛出来太多无意义停止词，所以加入停止词表 https://raw.githubusercontent.com/goto456/stopwords/master/baidu_stopwords.txt\n",
    "with open(\"baidu_stopwords.txt\") as f:\n",
    "    stop_word_list = f.read().splitlines()\n",
    "useless_str = '''一！“”，。？、；’\"',.、·《》（）()#\\t：\\n\\r\\n/'''\n",
    "\n",
    "words = hole_df['text'].progress_apply(lambda x: ' '.join([x for x in jieba.cut(x) if x not in stop_word_list and x not in useless_str]))\n",
    "hole_df['raw_text'] = hole_df['text'].copy()\n",
    "hole_df['text'] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "# hole_df = hole_df.loc[(hole_df.hour<=2) | (hole_df.hour>=23),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_df.loc[:,'hot_norm'] = hole_df.hot.apply(lambda x: x if x==0 else np.log(x))\n",
    "hole_df.loc[:,'hot_norm'] = hole_df.hot_norm.apply(lambda x: x if x!=0 else 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_hole = hole_df[hole_df.hot_norm>1]\n",
    "hot_hole.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfi_hole_adj = pd.concat([hole_df, hot_hole, hot_hole], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_vec = tfidf.fit_transform(tfi_hole_adj['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names from the vectorizer\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Get the sum of the tf-idf values for each feature across all documents\n",
    "sum_tfidf = np.sum(tfidf_vec.toarray(), axis=0)\n",
    "\n",
    "# Get the indices of the features sorted by their tf-idf sum in descending order\n",
    "sorted_indices = np.argsort(sum_tfidf)[::-1]\n",
    "\n",
    "# Print the top 10 features and their tf-idf sums\n",
    "print(\"Top 10 features and their tf-idf sums\")\n",
    "for i in range(10):\n",
    "    feature_index = sorted_indices[i]\n",
    "    feature_name = feature_names[feature_index]\n",
    "    tfidf_sum = sum_tfidf[feature_index]\n",
    "    print(f\"{feature_name}: {tfidf_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec vectorization for segmented '简介'\n",
    "sentences = [s.split() for s in hole_df['text']]\n",
    "\n",
    "seed = 123\n",
    "w2v = Word2Vec(sentences, vector_size=10, min_count=1, workers=4, seed=seed) # size is the dimensionality of the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in w2v.wv.most_similar(positive=['食堂'], topn=10):\n",
    "   print(e[0], e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first tokenize each sentence in the storyline column\n",
    "tokenized_sentences = hole_df.text.apply(lambda x: x.split())\n",
    "\n",
    "# We then calculate the word vectors for each word in each sentence\n",
    "word_vectors = tokenized_sentences.apply(lambda x: np.array([w2v.wv[word] for word in x if word in w2v.wv.key_to_index]))\n",
    "\n",
    "# We then calculate the average vector for each sentence\n",
    "sentence_vectors = word_vectors.apply(lambda x: np.mean(x, axis=0)) \n",
    "\n",
    "# The resulting sentence_vectors will contain the vector representation for each sentence in the storyline column.\n",
    "sentence_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_sentence_vectors = np.stack(np.array(sentence_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_sentence_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_df.loc[:,'hour_6am'] = hole_df.hour.apply(lambda x: x-6 if x-6 >= 0 else x-6+24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(hole_df[['reply', 'likenum', 'hour_6am']])\n",
    "train_data = np.hstack([train_data, w2v_sentence_vectors])\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "pca_train = PCA(n_components=0.95)\n",
    "train_data_pca = pca_train.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scree plot\n",
    "plt.plot(range(1, pca_train.n_components_ + 1), pca_train.explained_variance_ratio_.cumsum(), marker='o')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: K-means clustering\n",
    "\n",
    "# Elbow method to determine optimal number of clusters\n",
    "# Calculate the sum of squared errors for different values of k\n",
    "# Choose the value of k at the \"elbow\" of the plot\n",
    "# This is the point of diminishing returns, where adding more clusters doesn't significantly improve the SSE\n",
    "# Set the value of k to the optimal number of clusters\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate SSE for different values of k\n",
    "sse = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "    kmeans.fit(train_data_pca)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Plot SSE against number of clusters\n",
    "plt.plot(range(1, 11), sse)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()\n",
    "\n",
    "# Set the value of k to the optimal number of clusters\n",
    "k = 30\n",
    "\n",
    "# Perform K-means clustering with optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=114514, n_init='auto')\n",
    "kmeans.fit_transform(train_data_pca)\n",
    "# Use the trained KMeans model to predict the cluster labels for the test data\n",
    "labels = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the holes in each cluster\n",
    "cluster_holes = {}\n",
    "for i in range(k):\n",
    "    cluster_holes[i] = []\n",
    "\n",
    "# Iterate through the test data and add each movie to its corresponding cluster\n",
    "for i in range(len(train_data_pca)):\n",
    "    cluster_holes[labels[i]].append(hole_df.iloc[i])\n",
    "\n",
    "# Output representative movies, number of movies in each cluster, and rating distribution\n",
    "cluster_avg_hour_6am = []\n",
    "cluster_avg_like = []\n",
    "cluster_avg_reply = []\n",
    "for i in range(k):\n",
    "    print(\"====================\")\n",
    "    print(f\"Cluster {i+1}:\")\n",
    "    print(f\"Number of holes in cluster: {len(cluster_holes[i])}\")\n",
    "    print(f\"Avg time(start at 6am) {hole_df[labels == i]['hour_6am'].mean()}\")\n",
    "    cluster_avg_hour_6am.append(hole_df[labels == i]['hour_6am'].mean())\n",
    "    print(f\"Like mean: {hole_df[labels == i]['likenum'].mean()}\")\n",
    "    cluster_avg_like.append(hole_df[labels == i]['likenum'].mean())\n",
    "    print(f\"Reply mean: {hole_df[labels == i]['reply'].mean()}\")\n",
    "    cluster_avg_reply.append(hole_df[labels == i]['reply'].mean())\n",
    "    # Print the tags for each movie in the cluster\n",
    "    print(f\"Text for hole: {hole_df[labels == i]['raw_text'].sample(10)}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '/datb1/wuyuxuan/tmp/Holemonitor/data/2024-03-09_holes_monitor.db'\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    data = pd.read_sql(\"SELECT * FROM holes\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>time</th>\n",
       "      <th>reply</th>\n",
       "      <th>likenum</th>\n",
       "      <th>last_retrive</th>\n",
       "      <th>hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>6037913</td>\n",
       "      <td>popi  请问问题（以及dz可能回问）</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-10 01:01:55</td>\n",
       "      <td>92.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2024-03-10 01:27:01</td>\n",
       "      <td>736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>6037971</td>\n",
       "      <td>popi 👩 随便聊聊</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-10 01:32:15</td>\n",
       "      <td>99.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2024-03-10 02:20:22</td>\n",
       "      <td>693.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>6037332</td>\n",
       "      <td>洞主从22年12月“第一责任人”开始，因为家属认为新冠的后果还是要观察一段时间再下结论，就一...</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 22:05:02</td>\n",
       "      <td>33.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2024-03-09 22:23:19</td>\n",
       "      <td>462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>6037153</td>\n",
       "      <td>树洞的朋友们大家好。有一件事情想向大家求助，dz现在得了很严重的病，很难治好的那种。很多年前...</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 21:02:52</td>\n",
       "      <td>30.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2024-03-09 21:19:39</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>6037875</td>\n",
       "      <td>我很不喜欢北京。它现代，工整，四平八稳，过分地宽敞，显出一种冷漠来。冬春时是最讨厌的季节，刮...</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-10 00:44:22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2024-03-10 01:06:35</td>\n",
       "      <td>336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>6037421</td>\n",
       "      <td>原价20出一张周三晚上驾驶我的车 二楼中间靠后</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 22:33:37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-03-09 22:46:29</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>6037424</td>\n",
       "      <td>6036526捞捞 闲置 二手 出衣服 多买可刀</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 22:35:29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-03-09 22:49:11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>6037427</td>\n",
       "      <td>收留可爱妹妹～</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 22:36:20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-03-09 22:50:31</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>6037431</td>\n",
       "      <td>有点小痛苦</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 22:36:58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-03-09 22:50:31</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>6038034</td>\n",
       "      <td>悠悠浮云望穿\\r\\n人事看厌倦\\r\\n唯独情不变</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-10 02:50:15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-03-10 02:54:24</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1190 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pid                                               text  type   \n",
       "1068  6037913                               popi  请问问题（以及dz可能回问）  text  \\\n",
       "1126  6037971                                        popi 👩 随便聊聊  text   \n",
       "488   6037332  洞主从22年12月“第一责任人”开始，因为家属认为新冠的后果还是要观察一段时间再下结论，就一...  text   \n",
       "309   6037153  树洞的朋友们大家好。有一件事情想向大家求助，dz现在得了很严重的病，很难治好的那种。很多年前...  text   \n",
       "1030  6037875  我很不喜欢北京。它现代，工整，四平八稳，过分地宽敞，显出一种冷漠来。冬春时是最讨厌的季节，刮...  text   \n",
       "...       ...                                                ...   ...   \n",
       "577   6037421                            原价20出一张周三晚上驾驶我的车 二楼中间靠后  text   \n",
       "580   6037424                           6036526捞捞 闲置 二手 出衣服 多买可刀  text   \n",
       "583   6037427                                            收留可爱妹妹～  text   \n",
       "587   6037431                                              有点小痛苦  text   \n",
       "1189  6038034                           悠悠浮云望穿\\r\\n人事看厌倦\\r\\n唯独情不变  text   \n",
       "\n",
       "                     time  reply  likenum         last_retrive    hot  \n",
       "1068  2024-03-10 01:01:55   92.0      8.0  2024-03-10 01:27:01  736.0  \n",
       "1126  2024-03-10 01:32:15   99.0      7.0  2024-03-10 02:20:22  693.0  \n",
       "488   2024-03-09 22:05:02   33.0     14.0  2024-03-09 22:23:19  462.0  \n",
       "309   2024-03-09 21:02:52   30.0     12.0  2024-03-09 21:19:39  360.0  \n",
       "1030  2024-03-10 00:44:22   12.0     28.0  2024-03-10 01:06:35  336.0  \n",
       "...                   ...    ...      ...                  ...    ...  \n",
       "577   2024-03-09 22:33:37    0.0      1.0  2024-03-09 22:46:29    0.0  \n",
       "580   2024-03-09 22:35:29    0.0      0.0  2024-03-09 22:49:11    0.0  \n",
       "583   2024-03-09 22:36:20    0.0      1.0  2024-03-09 22:50:31    0.0  \n",
       "587   2024-03-09 22:36:58    0.0      1.0  2024-03-09 22:50:31    0.0  \n",
       "1189  2024-03-10 02:50:15    0.0      1.0  2024-03-10 02:54:24    0.0  \n",
       "\n",
       "[1190 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['reply'] = data.reply.astype(float)\n",
    "data['likenum'] = data.likenum.astype(float)\n",
    "data.loc[:,'hot'] = data.reply*data.likenum\n",
    "data.sort_values(by='hot', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '/datb1/wuyuxuan/tmp/Holemonitor/data/2024-03-09_holes_monitor.db'\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    data_com = pd.read_sql(\"SELECT * FROM comments\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>time</th>\n",
       "      <th>reply</th>\n",
       "      <th>likenum</th>\n",
       "      <th>last_retrive</th>\n",
       "      <th>hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>6037599</td>\n",
       "      <td>被nanpy甩了，难受，开个洞popi😭</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 23:25:56</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2024-03-09 23:37:57</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid                  text  type                 time  reply  likenum   \n",
       "755  6037599  被nanpy甩了，难受，开个洞popi😭  text  2024-03-09 23:25:56   28.0      6.0  \\\n",
       "\n",
       "            last_retrive    hot  \n",
       "755  2024-03-09 23:37:57  168.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.pid.isin(data_com[data_com.text.str.contains('fwb')].pid)].sort_values(by='time', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "6036940 deleted!\n",
    "6036940 deleted!\n",
    "6036940 deleted!\n",
    "6036940 deleted!\n",
    "6036940 deleted!\n",
    "6036940 deleted!\n",
    "6036940 deleted!\n",
    "6036940 deleted!\n",
    "6036940 deleted!\n",
    "6036980 deleted!\n",
    "6036940 deleted!\n",
    "6037025 deleted!\n",
    "6037171 deleted!\n",
    "6037180 deleted!\n",
    "6037258 deleted!\n",
    "6037367 deleted!\n",
    "6037345 deleted!\n",
    "6037812 deleted!\n",
    "6037876 deleted!\n",
    "6037921 deleted!\n",
    "6037937 deleted!\n",
    "\"\"\"\n",
    "deleted = re.findall(r'(\\d+) deleted!', s)\n",
    "deleted = [int(x) for x in deleted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>time</th>\n",
       "      <th>reply</th>\n",
       "      <th>likenum</th>\n",
       "      <th>last_retrive</th>\n",
       "      <th>hot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>6036940</td>\n",
       "      <td>聊聊最疯的经历？</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 19:44:25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-03-09 20:05:31</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6036980</td>\n",
       "      <td>蹲高一物理数学老师，地点大兴，时薪160。QQ6537️⃣7991️⃣8</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 20:02:20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2024-03-09 20:04:01</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>6037025</td>\n",
       "      <td>兼职 家教</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 20:14:49</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2024-03-09 20:16:44</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>6037171</td>\n",
       "      <td>None</td>\n",
       "      <td>image</td>\n",
       "      <td>2024-03-09 21:07:59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2024-03-09 21:14:10</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>6037180</td>\n",
       "      <td>这边有个高三数学的单，是那种线上讲解题目，按题给钱的。有同学想接吗，顺便可以说一下价格。</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 21:09:53</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2024-03-09 21:16:50</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>6037258</td>\n",
       "      <td>有木有uu认识的外校的uu愿意做助教的，线下改初中作业题，200/天</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 21:45:14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2024-03-09 21:49:00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>6037345</td>\n",
       "      <td>大四小语种\\n选一线城市公务员\\n还是海硕一年\\n（目前情况是，收到了港中文/港大和南洋理工...</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 22:11:34</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2024-03-09 22:21:54</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>6037367</td>\n",
       "      <td>昨天晚上让npy一夜cum了三次</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-09 22:19:03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2024-03-09 22:20:23</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>6037812</td>\n",
       "      <td>想wen</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-10 00:19:09</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2024-03-10 00:24:48</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>6037876</td>\n",
       "      <td>有没有直男想弯一下 找我</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-10 00:44:46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2024-03-10 00:49:04</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>6037921</td>\n",
       "      <td>北京大学医学部就是答辩\\n教书简直就是答辩\\n建议开除一些老师\\n花钱找点会教书育人的来不行...</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-10 01:05:26</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2024-03-10 01:10:55</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>6037937</td>\n",
       "      <td>树洞敏感词都有啥呀，我只是想发自己关于百元之恋的影评，为啥发不出来啊</td>\n",
       "      <td>text</td>\n",
       "      <td>2024-03-10 01:13:01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2024-03-10 01:21:10</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pid                                               text   type   \n",
       "99    6036940                                           聊聊最疯的经历？   text  \\\n",
       "137   6036980               蹲高一物理数学老师，地点大兴，时薪160。QQ6537️⃣7991️⃣8   text   \n",
       "182   6037025                                              兼职 家教   text   \n",
       "327   6037171                                               None  image   \n",
       "336   6037180       这边有个高三数学的单，是那种线上讲解题目，按题给钱的。有同学想接吗，顺便可以说一下价格。   text   \n",
       "414   6037258                 有木有uu认识的外校的uu愿意做助教的，线下改初中作业题，200/天   text   \n",
       "501   6037345  大四小语种\\n选一线城市公务员\\n还是海硕一年\\n（目前情况是，收到了港中文/港大和南洋理工...   text   \n",
       "523   6037367                                   昨天晚上让npy一夜cum了三次   text   \n",
       "967   6037812                                               想wen   text   \n",
       "1031  6037876                                       有没有直男想弯一下 找我   text   \n",
       "1076  6037921  北京大学医学部就是答辩\\n教书简直就是答辩\\n建议开除一些老师\\n花钱找点会教书育人的来不行...   text   \n",
       "1092  6037937                 树洞敏感词都有啥呀，我只是想发自己关于百元之恋的影评，为啥发不出来啊   text   \n",
       "\n",
       "                     time  reply  likenum         last_retrive    hot  \n",
       "99    2024-03-09 19:44:25    1.0      1.0  2024-03-09 20:05:31    1.0  \n",
       "137   2024-03-09 20:02:20    2.0      2.0  2024-03-09 20:04:01    4.0  \n",
       "182   2024-03-09 20:14:49    4.0      3.0  2024-03-09 20:16:44   12.0  \n",
       "327   2024-03-09 21:07:59    1.0      2.0  2024-03-09 21:14:10    2.0  \n",
       "336   2024-03-09 21:09:53   17.0      6.0  2024-03-09 21:16:50  102.0  \n",
       "414   2024-03-09 21:45:14    2.0      3.0  2024-03-09 21:49:00    6.0  \n",
       "501   2024-03-09 22:11:34    2.0      4.0  2024-03-09 22:21:54    8.0  \n",
       "523   2024-03-09 22:19:03    2.0      3.0  2024-03-09 22:20:23    6.0  \n",
       "967   2024-03-10 00:19:09    4.0      3.0  2024-03-10 00:24:48   12.0  \n",
       "1031  2024-03-10 00:44:46    1.0      2.0  2024-03-10 00:49:04    2.0  \n",
       "1076  2024-03-10 01:05:26    6.0      5.0  2024-03-10 01:10:55   30.0  \n",
       "1092  2024-03-10 01:13:01   10.0      5.0  2024-03-10 01:21:10   50.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.pid.isin(deleted)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "#由于筛出来太多无意义停止词，所以加入停止词表 https://raw.githubusercontent.com/goto456/stopwords/master/baidu_stopwords.txt\n",
    "with open(\"baidu_stopwords.txt\") as f:\n",
    "    stop_word_list = f.read().splitlines()\n",
    "useless_str = '''一！“”，。？、；’\"',.、·《》（）()#\\t：\\n\\r\\n/'''\n",
    "\n",
    "words = data['text'].progress_apply(lambda x: ' '.join([x for x in jieba.cut(x) if x not in stop_word_list and x not in useless_str]))\n",
    "data['raw_text'] = data['text'].copy()\n",
    "data['text'] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_vec = tfidf.fit_transform(data['text'])\n",
    "\n",
    "# Get the feature names from the vectorizer\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Get the sum of the tf-idf values for each feature across all documents\n",
    "sum_tfidf = np.sum(tfidf_vec.toarray(), axis=0)\n",
    "\n",
    "# Get the indices of the features sorted by their tf-idf sum in descending order\n",
    "sorted_indices = np.argsort(sum_tfidf)[::-1]\n",
    "\n",
    "# Print the top 10 features and their tf-idf sums\n",
    "print(\"Top 10 features and their tf-idf sums\")\n",
    "for i in range(10):\n",
    "    feature_index = sorted_indices[i]\n",
    "    feature_name = feature_names[feature_index]\n",
    "    tfidf_sum = sum_tfidf[feature_index]\n",
    "    print(f\"{feature_name}: {tfidf_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec vectorization for segmented '简介'\n",
    "sentences = [s.split() for s in data['text']]\n",
    "\n",
    "seed = 123\n",
    "w2v = Word2Vec(sentences, vector_size=10, min_count=1, workers=4, seed=seed) # size is the dimensionality of the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '../data/2023-06-14_holes_monitor.db'\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    data_holes = pd.read_sql(\"SELECT * FROM holes\", conn)\n",
    "    data_comments = pd.read_sql(\"SELECT * FROM comments\", conn)\n",
    "\n",
    "data_holes.to_csv(\"../data/614holes.csv\", index=True)\n",
    "data_comments.to_csv(\"../data/614comments.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '../data/2023-06-19_holes_monitor.db'\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    data = pd.read_sql(\"SELECT * FROM holes\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = \".*(短租|租房).*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.text.str.match(kw, flags=re.DOTALL)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
